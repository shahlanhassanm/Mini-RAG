<<<<<<< HEAD
# Mini RAG Assistant for Construction Marketplace

This is a Retrieval-Augmented Generation (RAG) system designed to answer user questions using internal documents (policies, FAQs, specifications). It demonstrates the core components of a RAG pipeline: document chunking, embedding, vector retrieval, and grounded answer generation with transparency.

## Features

- **Document Ingestion**: Loads PDF documents, chunks them, and creates embeddings.
- **Vector Search**: Uses FAISS (Facebook AI Similarity Search) for efficient semantic retrieval.
- **Grounded Generation**: Answers are generated strictly from the retrieved context.
- **Transparency**: Displays the retrieved document chunks alongside the final answer to ensure explainability.
- **Flexible LLM Support**: Supports both **OpenRouter** (for cloud models) and **Ollama** (for local open-source models).

## Prerequisites

- Python 3.8+
- [Ollama](https://ollama.com/) (Optional, for local LLM support)

## Setup

1.  **Clone the repository** (or navigate to the project folder):
    ```bash
    cd MiniRAG
    ```

2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Place Data**:
    - Put your PDF documents (policies, FAQs, specs) in the `data/` directory.

4.  **Configuration**:
    - Copy `.env.example` to `.env`:
        ```bash
        cp .env.example .env
    # On Windows: copy .env.example .env
        ```
    - **Option A: OpenRouter (Cloud)**
        - Get an API key from [OpenRouter](https://openrouter.ai/).
        - Set `LLM_TYPE=openrouter` in `.env`.
        - Set `OPENROUTER_API_KEY=your_key_here` in `.env`.
    - **Option B: Ollama (Local)**
        - Install Ollama and pull a model (e.g., `ollama pull llama3.2`).
        - Set `LLM_TYPE=ollama` in `.env`.
        - Set `OLLAMA_MODEL=llama3.2` (or your pulled model) in `.env`.
    
    > **Note**: If `ollama` command is not found after installation, try restarting your terminal or computer. You can also look for it in `C:\Users\%USERNAME%\AppData\Local\Programs\Ollama\ollama.exe`.

## Usage

### 1. Ingest Documents
Run this script to process the PDFs and build the vector index.
```bash
python ingest.py
```
This will create a `faiss_index` folder containing the vector store.

### 2. Run the Assistant
Start the interactive QA session.
```bash
python rag.py
```
Type your query when prompted. The assistant will show:
1.  **Retrieved Context**: The most relevant chunks from your documents.
2.  **Final Answer**: The answer generated by the LLM based on that context.

Type `exit` to quit.

## Design Choices

### Embedding Model
We use `sentence-transformers/all-MiniLM-L6-v2`.
- **Reason**: It is a lightweight, high-performance model that runs efficiently on CPU. It provides a good balance between speed and semantic search quality for English text.

### Vector Store
We use **FAISS**.
- **Reason**: It is the industry standard for efficient similarity search and clustering of dense vectors. The local file-based index fits the requirements perfectly without needing an external service.

### LLM
- **Default**: OpenRouter (access to free models like Llama 3 8B).
- **Bonus/Local**: Support for **Ollama** allows running models like Llama 3 or Mistral locally, ensuring data privacy and offline capability.

### Transparency & Grounding
The system explicitly shows the chunks retrieved for each query. The LLM prompt is engineered to strict constraints: "Answer the question based ONLY on the following context". This reduces hallucinations and allows the user to verify the source of the information.

## Quality Analysis & Observations

A comprehensive test suite (`test_rag.py`) was run with 15 queries covering various aspects of the documentation (Company Policies, Packages, Specs).

**Observations:**
- **Retrieval Accuracy**: The FAISS index with `all-MiniLM-L6-v2` embeddings demonstrated high accuracy in retrieving relevant chunks.
    - *Example*: Query "What is Indecimal?" correctly retrieved the Company Overview from `doc1.md`.
    - *Example*: Query "price per sqft for Premier" correctly retrieved the Pricing section from `doc2.md`.
- **Groundedness**: The system is designed to provide answers *only* from the retrieved context. When the relevant context is found, the LLM allows for precise extraction of facts (e.g., "₹1,995 /sqft").
- **Transparency**: The retrieval step explicitly prints the source file and content preview, allowing users to verify the information source immediately.

**Test Queries Covered:**
1. Company Overview & Differentiators
2. Package Pricing & Specifications (Steel, Cement, etc.)
3. Payment Policies & Escrow
4. Quality Assurance & Checkpoints
5. Customer Support & Maintenance

## Local vs. Cloud LLM Comparison (Bonus)

The system is architected to support both Cloud (OpenRouter) and Local (Ollama) LLMs.

| Feature | OpenRouter (Cloud) | Ollama (Local) |
| :--- | :--- | :--- |
| **Setup** | Easy (API Key required) | Moderate (Requires installing Ollama & downloading model) |
| **Privacy** | Data sent to external API | **100% Private**, data never leaves the machine |
| **Cost** | Free tiers available, but scaled usage costs money | **Free**, uses local compute resources |
| **Latency** | Network interaction adds latency (~1-3s) | Dependent on local hardware (CPU/GPU). Can be faster on GPU. |
| **Quality** | Access to SOTA models (Llama 3 70B, GPT-4o) | Limited by local hardware RAM (typically 7B-8B models) |

**Recommendation**: Use **Local LLM** for sensitive internal documents (Policies, Contracts) where privacy is paramount. Use **Cloud LLM** for development, testing, or when higher reasoning capabilities (larger models) are needed.
=======
# Mini RAG Assistant for Construction Marketplace

This is a Retrieval-Augmented Generation (RAG) system designed to answer user questions using internal documents (policies, FAQs, specifications). It demonstrates the core components of a RAG pipeline: document chunking, embedding, vector retrieval, and grounded answer generation with transparency.

## Features

- **Document Ingestion**: Loads PDF documents, chunks them, and creates embeddings.
- **Vector Search**: Uses FAISS (Facebook AI Similarity Search) for efficient semantic retrieval.
- **Grounded Generation**: Answers are generated strictly from the retrieved context.
- **Transparency**: Displays the retrieved document chunks alongside the final answer to ensure explainability.
- **Flexible LLM Support**: Supports both **OpenRouter** (for cloud models) and **Ollama** (for local open-source models).

## Prerequisites

- Python 3.8+
- [Ollama](https://ollama.com/) (Optional, for local LLM support)

## Setup

1.  **Clone the repository** (or navigate to the project folder):
    ```bash
    cd MiniRAG
    ```

2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Place Data**:
    - Put your PDF documents (policies, FAQs, specs) in the `data/` directory.

4.  **Configuration**:
    - Copy `.env.example` to `.env`:
        ```bash
        cp .env.example .env
    # On Windows: copy .env.example .env
        ```
    - **Option A: OpenRouter (Cloud)**
        - Get an API key from [OpenRouter](https://openrouter.ai/).
        - Set `LLM_TYPE=openrouter` in `.env`.
        - Set `OPENROUTER_API_KEY=your_key_here` in `.env`.
    - **Option B: Ollama (Local)**
        - Install Ollama and pull a model (e.g., `ollama pull llama3.2`).
        - Set `LLM_TYPE=ollama` in `.env`.
        - Set `OLLAMA_MODEL=llama3.2` (or your pulled model) in `.env`.
    
    > **Note**: If `ollama` command is not found after installation, try restarting your terminal or computer. You can also look for it in `C:\Users\%USERNAME%\AppData\Local\Programs\Ollama\ollama.exe`.

## Usage

### 1. Ingest Documents
Run this script to process the PDFs and build the vector index.
```bash
python ingest.py
```
This will create a `faiss_index` folder containing the vector store.

### 2. Run the Assistant
Start the interactive QA session.
```bash
python rag.py
```
Type your query when prompted. The assistant will show:
1.  **Retrieved Context**: The most relevant chunks from your documents.
2.  **Final Answer**: The answer generated by the LLM based on that context.

Type `exit` to quit.

## Design Choices

### Embedding Model
We use `sentence-transformers/all-MiniLM-L6-v2`.
- **Reason**: It is a lightweight, high-performance model that runs efficiently on CPU. It provides a good balance between speed and semantic search quality for English text.

### Vector Store
We use **FAISS**.
- **Reason**: It is the industry standard for efficient similarity search and clustering of dense vectors. The local file-based index fits the requirements perfectly without needing an external service.

### LLM
- **Default**: OpenRouter (access to free models like Llama 3 8B).
- **Bonus/Local**: Support for **Ollama** allows running models like Llama 3 or Mistral locally, ensuring data privacy and offline capability.

### Transparency & Grounding
The system explicitly shows the chunks retrieved for each query. The LLM prompt is engineered to strict constraints: "Answer the question based ONLY on the following context". This reduces hallucinations and allows the user to verify the source of the information.

## Quality Analysis & Observations

A comprehensive test suite (`test_rag.py`) was run with 15 queries covering various aspects of the documentation (Company Policies, Packages, Specs).

**Observations:**
- **Retrieval Accuracy**: The FAISS index with `all-MiniLM-L6-v2` embeddings demonstrated high accuracy in retrieving relevant chunks.
    - *Example*: Query "What is Indecimal?" correctly retrieved the Company Overview from `doc1.md`.
    - *Example*: Query "price per sqft for Premier" correctly retrieved the Pricing section from `doc2.md`.
- **Groundedness**: The system is designed to provide answers *only* from the retrieved context. When the relevant context is found, the LLM allows for precise extraction of facts (e.g., "₹1,995 /sqft").
- **Transparency**: The retrieval step explicitly prints the source file and content preview, allowing users to verify the information source immediately.

**Test Queries Covered:**
1. Company Overview & Differentiators
2. Package Pricing & Specifications (Steel, Cement, etc.)
3. Payment Policies & Escrow
4. Quality Assurance & Checkpoints
5. Customer Support & Maintenance

## Local vs. Cloud LLM Comparison (Bonus)

The system is architected to support both Cloud (OpenRouter) and Local (Ollama) LLMs.

| Feature | OpenRouter (Cloud) | Ollama (Local) |
| :--- | :--- | :--- |
| **Setup** | Easy (API Key required) | Moderate (Requires installing Ollama & downloading model) |
| **Privacy** | Data sent to external API | **100% Private**, data never leaves the machine |
| **Cost** | Free tiers available, but scaled usage costs money | **Free**, uses local compute resources |
| **Latency** | Network interaction adds latency (~1-3s) | Dependent on local hardware (CPU/GPU). Can be faster on GPU. |
| **Quality** | Access to SOTA models (Llama 3 70B, GPT-4o) | Limited by local hardware RAM (typically 7B-8B models) |

**Recommendation**: Use **Local LLM** for sensitive internal documents (Policies, Contracts) where privacy is paramount. Use **Cloud LLM** for development, testing, or when higher reasoning capabilities (larger models) are needed.
>>>>>>> d61fd08f0e71cee19c793e8d20e156cde1603872
